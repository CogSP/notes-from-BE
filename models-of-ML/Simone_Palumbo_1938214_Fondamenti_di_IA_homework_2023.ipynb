{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CogSP/uni-projects/blob/main/Simone_Palumbo_1938214_Fondamenti_di_IA_homework_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TspH1wJmAkdk"
      },
      "source": [
        "### For the homeworks we are going to use the \"[Online News Popularity Data Set](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#)\"\n",
        "\n",
        "The dataset can be used both for regression and classification tasks.\n",
        "\n",
        "#### Source:\n",
        "\n",
        "Kelwin Fernandes INESC TEC, Porto, Portugal/Universidade do Porto, Portugal.\n",
        "Pedro Vinagre ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Paulo Cortez ALGORITMI Research Centre, Universidade do Minho, Portugal\n",
        "Pedro Sernadela Universidade de Aveiro\n",
        "\n",
        "#### Data Set Information:\n",
        "\n",
        "* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\n",
        "* Acquisition date: January 8, 2015\n",
        "* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method. See their article for more details on how the relative performance values were set.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\n",
        "\n",
        "Attribute Information:\n",
        "0. url: URL of the article (non-predictive)\n",
        "1. timedelta: Days between the article publication and the dataset acquisition (non-predictive)\n",
        "2. n_tokens_title: Number of words in the title\n",
        "3. n_tokens_content: Number of words in the content\n",
        "4. n_unique_tokens: Rate of unique words in the content\n",
        "5. n_non_stop_words: Rate of non-stop words in the content\n",
        "6. n_non_stop_unique_tokens: Rate of unique non-stop words in the content\n",
        "7. num_hrefs: Number of links\n",
        "8. num_self_hrefs: Number of links to other articles published by Mashable\n",
        "9. num_imgs: Number of images\n",
        "10. num_videos: Number of videos\n",
        "11. average_token_length: Average length of the words in the content\n",
        "12. num_keywords: Number of keywords in the metadata\n",
        "13. data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n",
        "14. data_channel_is_entertainment: Is data channel 'Entertainment'?\n",
        "15. data_channel_is_bus: Is data channel 'Business'?\n",
        "16. data_channel_is_socmed: Is data channel 'Social Media'?\n",
        "17. data_channel_is_tech: Is data channel 'Tech'?\n",
        "18. data_channel_is_world: Is data channel 'World'?\n",
        "19. kw_min_min: Worst keyword (min. shares)\n",
        "20. kw_max_min: Worst keyword (max. shares)\n",
        "21. kw_avg_min: Worst keyword (avg. shares)\n",
        "22. kw_min_max: Best keyword (min. shares)\n",
        "23. kw_max_max: Best keyword (max. shares)\n",
        "24. kw_avg_max: Best keyword (avg. shares)\n",
        "25. kw_min_avg: Avg. keyword (min. shares)\n",
        "26. kw_max_avg: Avg. keyword (max. shares)\n",
        "27. kw_avg_avg: Avg. keyword (avg. shares)\n",
        "28. self_reference_min_shares: Min. shares of referenced articles in Mashable\n",
        "29. self_reference_max_shares: Max. shares of referenced articles in Mashable\n",
        "30. self_reference_avg_sharess: Avg. shares of referenced articles in Mashable\n",
        "31. weekday_is_monday: Was the article published on a Monday?\n",
        "32. weekday_is_tuesday: Was the article published on a Tuesday?\n",
        "33. weekday_is_wednesday: Was the article published on a Wednesday?\n",
        "34. weekday_is_thursday: Was the article published on a Thursday?\n",
        "35. weekday_is_friday: Was the article published on a Friday?\n",
        "36. weekday_is_saturday: Was the article published on a Saturday?\n",
        "37. weekday_is_sunday: Was the article published on a Sunday?\n",
        "38. is_weekend: Was the article published on the weekend?\n",
        "39. LDA_00: Closeness to LDA topic 0\n",
        "40. LDA_01: Closeness to LDA topic 1\n",
        "41. LDA_02: Closeness to LDA topic 2\n",
        "42. LDA_03: Closeness to LDA topic 3\n",
        "43. LDA_04: Closeness to LDA topic 4\n",
        "44. global_subjectivity: Text subjectivity\n",
        "45. global_sentiment_polarity: Text sentiment polarity\n",
        "46. global_rate_positive_words: Rate of positive words in the content\n",
        "47. global_rate_negative_words: Rate of negative words in the content\n",
        "48. rate_positive_words: Rate of positive words among non-neutral tokens\n",
        "49. rate_negative_words: Rate of negative words among non-neutral tokens\n",
        "50. avg_positive_polarity: Avg. polarity of positive words\n",
        "51. min_positive_polarity: Min. polarity of positive words\n",
        "52. max_positive_polarity: Max. polarity of positive words\n",
        "53. avg_negative_polarity: Avg. polarity of negative words\n",
        "54. min_negative_polarity: Min. polarity of negative words\n",
        "55. max_negative_polarity: Max. polarity of negative words\n",
        "56. title_subjectivity: Title subjectivity\n",
        "57. title_sentiment_polarity: Title polarity\n",
        "58. abs_title_subjectivity: Absolute subjectivity level\n",
        "59. abs_title_sentiment_polarity: Absolute polarity level\n",
        "60. shares: Number of shares (target)\n",
        "\n",
        "\n",
        "The first two columns (url and time_delta) are non-predictive and should be ignored\n",
        "\n",
        "The last column **shares** contains the value to predict.\n",
        "\n",
        "### Regression\n",
        "In the case of regression we want to predict the value of the share column.\n",
        "\n",
        "### Classification\n",
        "In the case of classification we want to predict one of two classes:\n",
        "\n",
        "* *low* -- shares < 1,400\n",
        "* *high* -- shares >= 1,400\n",
        "\n",
        "### Metrics\n",
        "\n",
        "#### Regression\n",
        "To evaluate how good we are doing on the **regression** task we will use the Root Mean Squared Error (RMSE). RMSE is given by\n",
        "\n",
        "$$\n",
        "\\sqrt{\\frac{1}{n}\\sum\\limits_{i=1}^{n}{\\Big(d_i -f_i\\Big)^2}}\n",
        "$$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "* $n$ is the number of test samples\n",
        "* $d_i$ is the ground truth value of the i-th sample\n",
        "* $f_i$ is the predicted value of the i-th sample\n",
        "\n",
        "\n",
        "#### Classification\n",
        "To evaluate how good we are doing on the **classification** task we will use the accuracy metrics. Accuracy is given by\n",
        "\n",
        "$$\n",
        "\\frac{TP+TN}{TP+TN+FP+FN}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* TP is the number of *correctly* classified positive samples\n",
        "* TN is the number of *correctly* classified negative samples\n",
        "* FP is the number of *incorrectly* classified positive samples\n",
        "* FN is the number of *incorrectly* classified negative samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oesd6_bYijRo",
        "outputId": "30f58fc5-984a-48ca-c047-6358bc53b65f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 13:12:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7476401 (7.1M) [application/x-httpd-php]\n",
            "Saving to: ‘OnlineNewsPopularity.zip.1’\n",
            "\n",
            "OnlineNewsPopularit 100%[===================>]   7.13M  11.2MB/s    in 0.6s    \n",
            "\n",
            "2023-05-06 13:12:51 (11.2 MB/s) - ‘OnlineNewsPopularity.zip.1’ saved [7476401/7476401]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00332/OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmad6QdZ_nFR",
        "outputId": "64a1e2a7-eaa9-454a-c58d-8645f9939c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  OnlineNewsPopularity.zip\n",
            "replace OnlineNewsPopularity/OnlineNewsPopularity.names? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip OnlineNewsPopularity.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduzione\n",
        "\n",
        "\n",
        "Alla fine c'è una cella contenente le tavole con le prestazioni di tutti i modelli, con una spiegazione sul perché le performance sono quelle e quale modello è migliore nelle varie situazioni\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dHUD6mi2esW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primo Modello: Alberi Decisionali\n",
        "Ho implementato un modello DecisionTree per il task di classificazione. \n",
        "\n",
        "## Data preprocessing\n",
        "- ho rimosso i primi due attributi dal dataframe, in quanto non sono predittivi. Ho droppato anche l'ultimo attributo, in quanto è il nostro target, e l'ho trasformato nella y rendendolo binario\n",
        "- ho splittato il dataset in train set, test set e validation set per fare hypertuning sull'iperparametro altezza. \n",
        "- ho **discretizzato** gli attributi per calcolare l'information gain. Ho provato i seguenti approcci:\n",
        "<ol type=\"a\">\n",
        "  <li>discretizzazione con media: hypertuning altezza 5, accuracy 0.6370286290831126</li>\n",
        "  <li>discretizzazione con mediana: hypertuning altezza 7, accuracy 0.6472442931012738</li>\n",
        "</ol>\n",
        "\n",
        "## Hypertuning\n",
        "Ho calcolato la migliore altezza in base all'accuracy utilizzando KFold di Sklearn. KFold di Sklearn si occupa dello splitting sul validation set.\n",
        "\n",
        "## Splitting dei nodi interni:\n",
        "Per costruire l'albero, bisogna trovare per ogni nodo interno qual è il miglior attributo per splittare nei due nodi figli. Il miglior attributo è quello che massimizza l'information gain. Poiché l'information gain misura l'incremento di informazione quando i samples di un nodo vengono suddivisi nei due nodi figli, l'attributo migliore è quello che meglio divide i dati nel nodo corrente. Come già detto utilizziamo media o mediana per discretizzare, e l'entropia per il calcolo dell'information gain.\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "I commenti nel codice spiegano le istruzioni che vengono eseguite. Le performance sono riportate dopo il codice.\n",
        "\n"
      ],
      "metadata": {
        "id": "fTL1_sO3xQlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, att=None, thr=None, left=None, right=None, value=None):\n",
        "        # ogni nodo mantiene\n",
        "        self.att = att  #l'informazione sull'attributo usato per lo split \n",
        "        self.thr = thr  # la soglia di splitting per l'attributo\n",
        "        self.left = left \n",
        "        self.right = right\n",
        "        self.value = value # label associata (solo se è foglia è != None)\n",
        "\n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, discretizzazione = \"mean\"):\n",
        "        self.n_features = None\n",
        "        self.root = None\n",
        "        self.max_depth = max_depth\n",
        "        self.discretizzazione = discretizzazione \n",
        "\n",
        "\n",
        "    # calcoliamo qual è la label che occorre di più\n",
        "    def plurality_value(self, y):\n",
        "        diz = {}\n",
        "        for c in y:\n",
        "            if c not in diz:\n",
        "                diz[c] = 0\n",
        "            diz[c] += 1\n",
        "\n",
        "        max_count = 0\n",
        "        plurality_value = None\n",
        "\n",
        "        for c in diz:\n",
        "            if diz[c] > max_count:\n",
        "                max_count = diz[c]\n",
        "                plurality_value = c\n",
        "        return Node(value=plurality_value)\n",
        "\n",
        "    def learn_decision_tree(self, X, y):\n",
        "\n",
        "        self.n_features = X.shape[1]\n",
        "        self.root = self.grow(X, y, None, 0)\n",
        "\n",
        "    def grow(self, X, y, parent_y, depth):\n",
        "\n",
        "        n_samples, n_feats = X.shape\n",
        "        n_labels = len(np.unique(y))\n",
        "\n",
        "\n",
        "        # se il nodo è \"vuoto\" ritorniamo la label che occorre più spesso nel padre\n",
        "        if n_samples == 0:\n",
        "            return self.plurality_value(parent_y)\n",
        "\n",
        "\n",
        "        # se il nodo contiene una sola label allora siamo in una foglia \n",
        "        elif n_labels == 1:\n",
        "            nodo = Node(value=y.iloc[0])\n",
        "            return nodo\n",
        "\n",
        "        # se il numero di features è 0 non possiamo andare avanti\n",
        "        # ritorniamo il plurality_value delle y che sono rimaste\n",
        "        elif n_feats == 0:\n",
        "            return self.plurality_value(y)\n",
        "\n",
        "        # se siamo arrivati alla profondità massima concessa ci fermiamo \n",
        "        if self.max_depth != None and depth >= self.max_depth:\n",
        "            \n",
        "            return self.plurality_value(parent_y)\n",
        "\n",
        "        \n",
        "        best_feature, best_thresh = self.importance(X, y, n_feats)\n",
        "\n",
        "        # abbiamo ottenuto il best split, ora applichiamo grow() ricorsivamente sui due figli\n",
        "        left_idxs, right_idxs = self._split(X.iloc[:, best_feature], best_thresh)\n",
        "        left = self.grow(X.iloc[left_idxs, :], y.iloc[left_idxs], y, depth + 1)\n",
        "        right = self.grow(X.iloc[right_idxs, :], y.iloc[right_idxs], y, depth + 1)\n",
        "\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "    def importance(self, X, y, feat_idxs):\n",
        "\n",
        "        best_gain = -1\n",
        "        split_idx, split_threshold = None, None\n",
        "\n",
        "\n",
        "        # Adesso calcolo l'information gain. Si tratta di una misura dell'incremento di informazione \n",
        "        # quando i samples di un nodo vengono suddivisi nei due nodi figli. Vogliamo massimizzarlo \n",
        "        # per ottenere una suddivisione dei dati il più informativa possibile.\n",
        "        # Per massimizzarlo andiamo a trovare qual è l'attributo con information gain più alto,\n",
        "        # cioè l'attributo che meglio divide i dati correnti\n",
        "\n",
        "        # ho inserito la possibilità di usare sia la mediana che la media come metodi \n",
        "        # di discretizzazione\n",
        "        for feat_idx in range(feat_idxs):\n",
        "\n",
        "            X_column = X.iloc[:, feat_idx]\n",
        "            \n",
        "            \n",
        "            #discretizzazione con mediana\n",
        "            \n",
        "            if (self.discretizzazione == \"median\"):\n",
        "              threshold = np.median(X_column)\n",
        "\n",
        "            #discretizzazione con media\n",
        "            if (self.discretizzazione == \"mean\"): \n",
        "              threshold = np.mean(X_column)\n",
        "            \n",
        "\n",
        "            gain = self._information_gain(y, X_column, threshold)\n",
        "\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                split_idx = feat_idx\n",
        "                split_threshold = threshold\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "    def _information_gain(self, y, X_column, threshold):\n",
        "\n",
        "        parent_entropy = self._entropy(y)\n",
        "\n",
        "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
        "\n",
        "        # se o il figlio destro o il figlio sinistro non hanno samples l'information gain è 0 in quanto\n",
        "        # l'entropia del padre è uguale a quella del figlio non vuoto\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "\n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "\n",
        "        # calcoliamo l'entropia dei due figli\n",
        "        e_l, e_r = self._entropy(y.iloc[left_idxs]), self._entropy(y.iloc[right_idxs])\n",
        "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
        "\n",
        "        # l'information_gain è definito come la differenza tra l'entropia del padre e dei figli\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "\n",
        "        return information_gain\n",
        "\n",
        "    def _split(self, X_column, split_thresh):\n",
        "\n",
        "        # left_idxs conterrà gli indici dei sample il cui valore dell'attributo di X_column \n",
        "        # è minore della soglia. Il contrario per right_idxs.\n",
        "\n",
        "        left_idxs = np.where(X_column <= split_thresh)[0]\n",
        "        right_idxs = np.where(X_column > split_thresh)[0]\n",
        "        return left_idxs, right_idxs\n",
        "\n",
        "    def _entropy(self, y):\n",
        "        p_x = np.bincount(y) / len(y)  \n",
        "        return -np.sum([p * np.log(p) for p in p_x if p > 0])\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        predictions = []\n",
        "        \n",
        "        # per ogni riga percorriamo l'albero e troviamo in questo modo\n",
        "        # qual è la label associata (node.value della foglia in cui arriviamo)\n",
        "        for index, row in X.iterrows():\n",
        "            predictions.append(self._traverse_tree(row, self.root))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _traverse_tree(self, x, node, depth=0):\n",
        "      \n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x.iloc[node.att] <= node.thr:\n",
        "            return self._traverse_tree(x, node.left, depth + 1)\n",
        "        return self._traverse_tree(x, node.right, depth + 1)\n",
        "\n",
        "\n",
        "# effettuiamo K fold cross validation per trovare il valore \n",
        "# migliore dell'altezza. \n",
        "def KFold_cross_validation(X, y, max_depth=None):\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idxs, test_idxs in kf.split(X):\n",
        "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
        "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
        "\n",
        "        clf = DecisionTree(max_depth)\n",
        "        clf.learn_decision_tree(X_train, y_train)\n",
        "\n",
        "        predictions = clf.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    print(\"\\t per altezza:\", max_depth, \"l'accuracy media è:\", mean_acc)\n",
        "    return mean_acc\n",
        "\n",
        "\n",
        "def clean_and_split_dataset(df):\n",
        "\n",
        "    df = df.rename(columns=lambda x: x.strip())\n",
        "    df = df.iloc[:, 2:]\n",
        "\n",
        "    X = df.drop(\"shares\", axis=1)\n",
        "\n",
        "    y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=1938214)\n",
        "\n",
        "\n",
        "# hypertuning su l'altezza\n",
        "def model_selection(X, y):\n",
        "        \n",
        "    best_accuracy = -1\n",
        "    altezza = 0\n",
        "\n",
        "    while True:\n",
        "\n",
        "      altezza += 1\n",
        "\n",
        "      accuracy = KFold_cross_validation(X_train, y_train, max_depth=altezza)\n",
        "\n",
        "      if (accuracy < best_accuracy):\n",
        "        break\n",
        "\n",
        "      best_accuracy = accuracy\n",
        "      best_altezza = altezza\n",
        "    \n",
        "    return best_altezza\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = clean_and_split_dataset(df)\n",
        "\n",
        "\n",
        "print(\"hypertuning altezza:\",)\n",
        "\n",
        "best_altezza = model_selection(X_train, y_train)\n",
        "\n",
        "print(\"best_altezza:\", best_altezza, \"\\n\")\n",
        "\n",
        "clf = DecisionTree(max_depth=best_altezza, discretizzazione = \"mean\")\n",
        "clf.learn_decision_tree(X_train, y_train)\n",
        "predictions = clf.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMPI2__mxRHf",
        "outputId": "35ecc5c0-d13b-4d7f-97ee-836725c856c1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hypertuning altezza:\n",
            "\t per altezza: 1 l'accuracy media è: 0.5318618950023648\n",
            "\t per altezza: 2 l'accuracy media è: 0.5810184455305061\n",
            "\t per altezza: 3 l'accuracy media è: 0.6081349519154975\n",
            "\t per altezza: 4 l'accuracy media è: 0.6276209995270376\n",
            "\t per altezza: 5 l'accuracy media è: 0.6398549582216616\n",
            "\t per altezza: 6 l'accuracy media è: 0.6380892322244994\n",
            "best_altezza: 5 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Alberi di Decisione"
      ],
      "metadata": {
        "id": "MxEVS5vH-ZD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy: \n"
      ],
      "metadata": {
        "id": "9stbgNJg6ZXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, predictions)\n",
        "print(\"accuracy del modello sul test:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgIVmHy7_kna",
        "outputId": "ab5ece7b-2264-4f5b-fbec-12c3fa0f9b4d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy del modello sul test: 0.6370286290831126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix: considerando 0 la classe positiva e 1 la classe negativa, abbiamo\n",
        "- In alto a sinistra i True Positive (TP): samples per cui il modello ha previsto correttamente la classe positiva \n",
        "- In alto a destra i False Positive (FP): samples per cui il modello ha erroneamente previsto la classe positiva\n",
        "- In basso a sinistra i False Negative (FN): samples per cui il modello ha erroneamente previso la classe negativa\n",
        "- In basso a destra i True Negative (TN): samples per cui il modello ha correttamente previsto la classe negativa\n"
      ],
      "metadata": {
        "id": "0qZmMmNz_q4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cm(data):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.matshow(data, cmap='Reds')\n",
        "  for (i, j), z in np.ndenumerate(data):\n",
        "    ax.text(j, i, '{0:0.1f}'.format(z), ha='center', va='center')\n",
        "  plt.xlabel(\"y_pred\")\n",
        "  plt.ylabel(\"y_true\")\n",
        "  plt.show()\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "plot_cm(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "pvZtljq8_meY",
        "outputId": "a82111e2-863b-4d99-c0a4-62f31c007d28"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TODO: AGGIUNGI LE ALTRE METRICHE"
      ],
      "metadata": {
        "id": "l-ZDhyM_GCJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confronto con Sklearn\n",
        "\n",
        "Le prestazioni del modello di sklearn sono comparabili con quelle del mio modello"
      ],
      "metadata": {
        "id": "3Gv7JfVq345G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import tree\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "\n",
        "\n",
        "def KFold_cross_validation(X, y, max_depth=None):\n",
        "\n",
        "    kf = KFold(n_splits=5)\n",
        "\n",
        "    accuracies = []\n",
        "\n",
        "    for train_idxs, test_idxs in kf.split(X):\n",
        "        X_train, X_test = X.iloc[train_idxs], X.iloc[test_idxs]\n",
        "        y_train, y_test = y.iloc[train_idxs], y.iloc[test_idxs]\n",
        "\n",
        "        clf = tree.DecisionTreeClassifier(max_depth = max_depth)\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        predictions = clf.predict(X_test)\n",
        "        acc = accuracy_score(y_test, predictions)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "    mean_acc = np.mean(accuracies)\n",
        "    #print(\"per altezza:\", max_depth, \"l'accuracy media è:\", mean_acc)\n",
        "    return mean_acc\n",
        "\n",
        "\n",
        "def clean_and_split_dataset(df):\n",
        "\n",
        "    df = df.rename(columns=lambda x: x.strip())\n",
        "    df = df.iloc[:, 2:]\n",
        "\n",
        "    X = df.drop(\"shares\", axis=1)\n",
        "\n",
        "    y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=1938214)\n",
        "\n",
        "\n",
        "# hypertuning su l'altezza\n",
        "def model_selection(X, y):\n",
        "        \n",
        "    best_accuracy = -1\n",
        "    altezza = 0\n",
        "\n",
        "    while True:\n",
        "\n",
        "      altezza += 1\n",
        "\n",
        "      accuracy = KFold_cross_validation(X_train, y_train, max_depth=altezza)\n",
        "\n",
        "      if (accuracy < best_accuracy):\n",
        "        break\n",
        "\n",
        "      best_accuracy = accuracy\n",
        "      best_altezza = altezza\n",
        "    \n",
        "    return best_altezza\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = clean_and_split_dataset(df)\n",
        "\n",
        "best_altezza = model_selection(X_train, y_train)\n",
        "\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(max_depth=best_altezza)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "predictions = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, predictions)\n",
        "\n",
        "\n",
        "print(\"accuracy del modello sul test:\", acc)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "def plot_cm(data):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.matshow(data, cmap='Reds')\n",
        "  for (i, j), z in np.ndenumerate(data):\n",
        "    ax.text(j, i, '{0:0.1f}'.format(z), ha='center', va='center')\n",
        "  plt.xlabel(\"y_pred\")\n",
        "  plt.ylabel(\"y_true\")\n",
        "  plt.show()\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "plot_cm(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "OvnbQXSQ4BGl",
        "outputId": "8fcedf94-ccef-4114-a132-f29b373bb8d8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy del modello sul test: 0.6468659351746753\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHdDTKxD2V7B"
      },
      "source": [
        "# Secondo Modello: Regressione Lineare\n",
        "Ho implementato la regressione lineare per il task di regressione senza regolarizzazione e con regolarizzazioni L1, L2 ed Elastic Net. Ho inoltre implementato la regressione lineare per task di classificazione.\n",
        "\n",
        "## Data preprocessing\n",
        "- Ho pulito il dataset come per il modello precedente\n",
        "- Ho utilizzato StandardScaler() di sklearn per normalizzare i dati ed evitare overflow\n",
        "- ho aggiunto una colonna di tutti 1 ad X, così da poter inglobare il bias in W. Alternativamente, avrei dovuto scrivere  y_pred = np.dot(X, self.weights) + bias \n",
        "\n",
        "\n",
        "## Gradient Descent\n",
        "Ho utilizzato l'algoritmo di Gradient Descent per trovare i migliori parametri W che minimizzano l'errore medio. L'algoritmo calcola iterativamente il gradiente della funzione di costo rispetto ai parametri W e aggiornando questi parametri utilizzando un learning rate che  controlla la dimensione dei passi di aggiornamento. Ho deciso di impostare un numero massimo di epoch dopo cui la discesa termina. \n",
        "\n",
        "\n",
        "## Hypertuning\n",
        "Poichè i tempi di esecuzione superavano i 20 minuti, ho deciso di non inserire hypertuning nei modelli di linear regression. Comunque, sarebbe stato possibile fare K Fold cross validation per trovare i migliori valori di learning rate, n_iter (epoch) e tasso di regolarizzazione con le seguenti funzioni:\n",
        "\n",
        "\n",
        "```python\n",
        "def KFold_cross_validation(X, y, alpha, n_iter, l):\n",
        "    kf = KFold(n_splits=5)\n",
        "    rmses = []\n",
        "    for (train_idxs, test_idxs) in kf.split(X):\n",
        "        (X_train, X_test) = (X[train_idxs], X[test_idxs])\n",
        "        (y_train, y_test) = (y.iloc[train_idxs], y.iloc[test_idxs])\n",
        "        clf = MyLasso(alpha=alpha, n_iters=n_iter, l=l)\n",
        "        clf.fit(X_train, y_train)\n",
        "        predictions = clf.predict(X_test)\n",
        "        rmse = math.sqrt(mean_squared_error(y_test, predictions))\n",
        "        rmses.append(rmse)\n",
        "    mean_rmse = np.mean(rmses)\n",
        "    return mean_rmse\n",
        "\n",
        "def model_selection(X, y, iperparametri):\n",
        "    best_rmse = 100000\n",
        "    for a in iperparametri['alpha']:\n",
        "        for ni in iperparametri['n_iter']:\n",
        "            for l in iperparametri['lambda']:\n",
        "                rmse = KFold_cross_validation(X_train, y_train, alpha=a, n_iter=ni, l=l)\n",
        "                if rmse > best_rmse:\n",
        "                    break\n",
        "                best_rmse = rmse\n",
        "                best_alpha = a\n",
        "                best_n_iter = ni\n",
        "                best_l = l\n",
        "                print ('\\t best_rmse:', rmse, ' best_alpha:', a, ' best_n_iter:', ni, ' lambda:', l)\n",
        "    return (best_alpha, best_n_iter, best_l, best_rmse)\n",
        "\n",
        "\n",
        "    # prima di trainare il modello sul training set\n",
        "    iperparametri = {'alpha': [0.001, 0.01, 0.1, 1], 'n_iter': [10, 100, 1000, 10000],'lambda': [0.1, 1, 5, 10]}\n",
        "    best_alpha, best_n_iter, best_l, best_rmse = model_selection(X_train, y_train, iperparametri)\n",
        "    print ('best_alpha: ', best_alpha, 'best_n_iter: ', best_n_iter, 'best_l:' best_l, 'con rmse:', best_rmse)\n",
        "```\n",
        "\n",
        "<br>\n",
        "<hr>\n",
        "<br>\n",
        "\n",
        "\n",
        "I commenti nel codice spiegano le istruzioni che vengono eseguite. Le performance sono riportate dopo il codice.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression senza regolarizzazione"
      ],
      "metadata": {
        "id": "rFFSzICRHoV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "class MyLinearRegression:\n",
        "\n",
        "    def __init__(self, alpha=0.001, n_iters=10000):\n",
        "        self.alpha = alpha\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None  \n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples, n_features = X.shape  \n",
        "\n",
        "        # inizializziamo i pesi a 0\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "\n",
        "        # facciamo gradient descent       \n",
        "        for i in range(self.n_iters):\n",
        "       \n",
        "\n",
        "          # calcoliamo la previsione del modello\n",
        "          y_pred = np.dot(X, self.weights)\n",
        "\n",
        "\n",
        "          # calcoliamo il gradiente dell'errore medio \n",
        "          # rispetto alla previsione appena fatta\n",
        "          grad = (np.dot(X.transpose(), (y - y_pred))) / n_samples    \n",
        "       \n",
        "          # aggiornamento dei pesi \n",
        "          self.weights = self.weights + self.alpha * grad\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.dot(X, self.weights)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "\n",
        "# normalizzazione\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# aggiungo la colonna di 1\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])  \n",
        "\n",
        "reg = MyLinearRegression()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "nboV0zLtGK1r"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Linear Regression senza regolarizzazione"
      ],
      "metadata": {
        "id": "pfl4i-FxNy3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE:"
      ],
      "metadata": {
        "id": "c5Bdw6mOOBlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEtyDmifGKyc",
        "outputId": "c0c50a95-0a99-4d84-e771-4530a58b80e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7647.028265098518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: AGGIUNGI ALTRE METRICHE"
      ],
      "metadata": {
        "id": "oCyXnB90OHtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confronto con sklearn"
      ],
      "metadata": {
        "id": "JloMXky1OTyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1)  \n",
        "\n",
        "y = df[\"shares\"] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v5JIABIGJ_g",
        "outputId": "71b85a95-12f0-43ad-d6d1-c817366c2621"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7643.522429556329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression con regolarizzazione L1 (Lasso)\n",
        "\n",
        "\n",
        "Usiamo la formula:\n",
        "$ \\nabla = \\frac{X^T \\cdot (y_{pred} - y) +\\lambda_{1} sign(w)}{n_{samples}} $    \n",
        "\n",
        "#### **Perché usare la regolarizzazione L1?**\n",
        "        \n",
        "Per modelli di regressione lineare **multivariabile** come il nostro, è possibile che alcune dimensioni (alcuni dei 58 attributi) che in generale sono irrilevanti si rivelino per caso \"utili\" in un'istanza di training. Si tratta quindi di un caso di overfitting, che possiamo mitigare con la regolarizzazione L1. Questa regolarizzazione, infatti, produce un *modello sparso* che azzera i valori dei pesi in W associati ad attributi irrilevanti. In questo modo si vanno a selezionare le caratteristiche più importanti del modello, diminuendo la complessità e quindi fornendo un modello più semplice e più interpretabile da un umano."
      ],
      "metadata": {
        "id": "4jIrwX_COy7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Cambiando soltanto la formula del gradiente rispetto al modello precedente, non commento le istruzioni eseguite \n",
        "\n",
        "class MyLasso:\n",
        "\n",
        "    def __init__(self, alpha=0.001, n_iters=10000, l=5):\n",
        "        self.alpha = alpha\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None  \n",
        "        self.l = l\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples, n_features = X.shape  \n",
        "\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "       \n",
        "          y_pred = np.dot(X, self.weights)\n",
        "\n",
        "          grad = (np.dot(X.transpose(), (y_pred - y)) + (self.l)*np.sign(self.weights)) / n_samples    \n",
        "       \n",
        "          self.weights = self.weights - self.alpha * grad\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.dot(X, self.weights)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
        "\n",
        "\n",
        "reg = MyLasso()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "ecblnjcBPN4U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance per Lasso"
      ],
      "metadata": {
        "id": "9RO_Cn5qBIpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBGsxBJcBK5e",
        "outputId": "2292c09d-1f53-494f-d347-100ed7921ebd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7647.028264033903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "qmAGDVopPOho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"] \n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "model = linear_model.Lasso(alpha = 5) \n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "id": "8yruaOGKOzR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee73f6c6-ab27-467c-fe0c-196d90d414c7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7644.721121302028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression con regolarizzazione L2 (Ridge)\n",
        "\n",
        "\n",
        "Usiamo la formula:\n",
        "$ \\nabla = \\frac{X^T \\cdot (y_{pred} - y) + \\lambda_{2} w}{n_{samples}} $    \n",
        "\n",
        "#### **Perché usare la regolarizzazione L2?**\n",
        "Come la regolarizzazione L1, L2 cerca di evitare l'overfitting, ma non azzera i valori dei pesi associati a dimensioni irrilevanti, bensì ha l'obiettivo di rendere piccoli i parametri, cioè normalizzarli, disponendoli su un cerchio di raggio 1, così da non avere parametri molto più grandi di altri.\n"
      ],
      "metadata": {
        "id": "2zDo3WvZPUvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Cambiando soltanto la formula del gradiente rispetto al modello precedente, non commento le istruzioni eseguite\n",
        "\n",
        "class MyRidge:\n",
        "\n",
        "    def __init__(self, alpha=0.001, n_iters=10000, l=0.5):\n",
        "        self.alpha = alpha\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None  \n",
        "        self.l = l\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples, n_features = X.shape  \n",
        "\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "       \n",
        "          y_pred = np.dot(X, self.weights)\n",
        "\n",
        "          grad = (np.dot(X.transpose(), (y_pred - y)) + (self.l)*self.weights) / n_samples    \n",
        "       \n",
        "          self.weights = self.weights - self.alpha * grad\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "        y_pred = np.dot(X, self.weights)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "\n",
        "\n",
        "reg = MyRidge()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "  \n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "9xvokHNhPg04"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Ridge"
      ],
      "metadata": {
        "id": "fgb1JTRlBbsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMlWwpOwBdjV",
        "outputId": "199e9856-7153-45b4-88f2-c09d3d68e386"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7647.027603037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "x_kDZCELPXCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "reg = linear_model.Ridge(alpha = 0.5)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "print(\"weights\", reg.coef_, \"\\n\\n\\n\")\n",
        "print(\"intercept w_0\", reg.intercept_, \"\\n\\n\\n\")\n",
        "  \n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print(\"y_pred:\", y_pred, \"\\n\\n\\n\")\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "id": "iOV4Z1r9PdB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c576f43d-4c37-4ac2-ae6a-e841435b5f21"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weights [ 7.48373935e+01  5.79662639e-01  3.92032556e+03 -1.37185667e+03\n",
            " -2.01986866e+03  2.84892427e+01 -5.91478602e+01  1.42389509e+01\n",
            "  2.94820599e+00 -5.96136127e+02  3.32021558e+01 -9.37167256e+02\n",
            " -1.04248839e+03 -6.51755465e+02 -5.90707711e+02 -3.43809399e+02\n",
            " -2.98792266e+02  1.78295313e+00  9.08750135e-02 -4.09428693e-01\n",
            " -1.49193746e-03 -6.27763523e-04 -1.28749768e-03 -3.74935022e-01\n",
            " -2.01746277e-01  1.73098044e+00  2.90644039e-02  7.99395977e-03\n",
            " -9.52723722e-03  3.54536772e+02 -1.48227237e+02 -1.02031280e+02\n",
            " -2.28900887e+02 -9.98464981e+01  4.13092615e+02 -1.88623485e+02\n",
            "  2.24469130e+02  6.91849723e+02 -2.42732077e+02 -7.21422226e+02\n",
            "  1.75961773e+02  1.02815131e+02  2.68799080e+03  3.80311409e+02\n",
            " -1.29505076e+04 -1.28947091e+03  2.32809893e+03  2.25034917e+03\n",
            " -1.68277584e+03 -1.82778200e+03  3.40530600e+02 -2.14712865e+03\n",
            "  2.47813541e+02  2.37991210e+02 -3.11536829e+02  1.00973941e+02\n",
            "  5.27735382e+02  8.54158990e+02] \n",
            "\n",
            "\n",
            "\n",
            "intercept w_0 -536.4771594403423 \n",
            "\n",
            "\n",
            "\n",
            "y_pred: [5534.9631668  2194.07219648 3211.38224211 ... 8641.00612453 2037.82333856\n",
            " 4393.87283075] \n",
            "\n",
            "\n",
            "\n",
            "rmse: 7643.193830354128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression con regolarizzazione L1 ed L2 (Elastic Net)\n",
        "\n",
        "Ottenuta combinando la regolarizzazione L1 e la regolarizzazione L2. \n",
        "<br>\n",
        "La formula del gradiente è quindi:\n",
        "$ \\nabla = \\frac{X^T \\cdot (y_{pred} - y) + \\lambda_{1} sign(w) + \\lambda_{2} w}{n_{samples}} $    \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "b8ppHuYrPmY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Cambiando soltanto la formula del gradiente rispetto al modello precedente, non commento le istruzioni eseguite\n",
        "\n",
        "\n",
        "class MyElasticNet:\n",
        "\n",
        "    def __init__(self, alpha=0.1, n_iters=1000, l1 = 0.5, l2 = 0.5):\n",
        "        self.alpha = alpha\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None  \n",
        "        self.l1 = l1\n",
        "        self.l2 = l2\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples, n_features = X.shape  \n",
        "\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "       \n",
        "          y_pred = np.dot(X, self.weights)\n",
        "\n",
        "          grad = (np.dot(X.transpose(), (y_pred - y)) + self.l1 * np.sign(self.weights) + self.l2 * self.weights) / n_samples    \n",
        "            \n",
        "          self.weights = self.weights - self.alpha * grad\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "        y_pred = np.dot(X, self.weights)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "\n",
        "\n",
        "reg = MyElasticNet()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "fx990oytP-XJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Elastic Net"
      ],
      "metadata": {
        "id": "bffqi3WuBzpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81N2NBpEB1Rn",
        "outputId": "8fc31ee9-e8c2-4c43-df20-2aad6c3fe527"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7645.293042978001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "xMOIeK2yP5ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "reg = linear_model.ElasticNet()\n",
        "reg.fit(X_train, y_train)\n",
        "  \n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "id": "ft7zwP8GP6ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c7001ee-d5a9-4477-bfb3-ee449e4edfe0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7655.99517721312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificazione con Linear Regression\n",
        "\n",
        "Utilizziamo un modello di regressione lineare per risolvere il task di classificazione. Abbiamo quindi bisogno di un *decision boundary* che separi le due classi.\n",
        "\n",
        "#### **Threshold Function**\n",
        "Per separare linearmente i dati, la nostra ipotesi h diventa la seguente funzione:\n",
        "\n",
        "\\begin{equation}\n",
        "  h_w(x) = Threshold(z) = \\begin{cases}\n",
        "      1 & \\text{se } z \\geq 0 \\\\\n",
        "      0 & \\text{altrimenti}\n",
        "  \\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "con $z = w \\cdot x$\n",
        "\n"
      ],
      "metadata": {
        "id": "WAVjBXZoQYm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def step(x):\n",
        "    return np.heaviside(x, 0.5)\n",
        "x = np.linspace(-5, 5, 1000)\n",
        "y = step(x)\n",
        "plt.plot(x, y)\n",
        "plt.title('Separatore Lineare')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('h')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "j9Z62MgLStCv",
        "outputId": "ad4fa936-ab52-48f3-adb0-089876e0f24c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Data la difficoltà del problema, il seguente modello produce un accuracy sotto il 60%. Utilizzare una threshold meno \"netta\", come si vedrà con il modello successivo, migliora le prestazioni."
      ],
      "metadata": {
        "id": "-vxb2CPCSsOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "class MyLinearClassifier:\n",
        "\n",
        "    def __init__(self, alpha=0.01, n_iters=1000):\n",
        "        self.alpha = alpha\n",
        "        self.n_iters = n_iters\n",
        "        self.weights = None  \n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        n_samples, n_features = X.shape  \n",
        "\n",
        "        self.weights = np.zeros(n_features)\n",
        "\n",
        "        for i in range(self.n_iters):\n",
        "\n",
        "          example = X\n",
        "\n",
        "          examples_target = y\n",
        "\n",
        "          y_pred_example = []\n",
        "\n",
        "          # calcolo della funzione di threshold\n",
        "          for k in range(example.shape[0]):\n",
        "\n",
        "            w_x = np.dot(self.weights, example[k])\n",
        "\n",
        "            # if w*x >= 0 classe 1, altrimenti classe 0 \n",
        "            if (w_x >= 0): \n",
        "              y_pred_example.append(1)\n",
        "            else:\n",
        "              y_pred_example.append(0)\n",
        "\n",
        "          # aggiornamento dei pesi eseguito utilizzando l'errore tra i valori binari di ground truth e quelli appena predetti   \n",
        "          self.weights = self.weights + self.alpha * np.matmul((examples_target - y_pred_example), example)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "      \n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "        y_pred = []\n",
        "\n",
        "        for i in range(X.shape[0]):\n",
        "          z = np.dot(self.weights, X[i])\n",
        "          if (z >= 0):\n",
        "            y_pred.append(1)\n",
        "          else:\n",
        "            y_pred.append(0)\n",
        "         \n",
        "        return y_pred\n",
        "    \n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "\n",
        "\n",
        "reg = MyLinearClassifier()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "8xMp0_LIQlEr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prestazioni Linear Regression per task di Classificazione"
      ],
      "metadata": {
        "id": "t8dFeq27M-4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"acc:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSyxRyxqNBxq",
        "outputId": "bb62a4d8-3f30-4dd8-fdce-cc67506ead29"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.5855719510657081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "hr-DyuxRQc5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "y = df[\"shares\"] \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x > 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "reg = SGDClassifier()\n",
        "reg.fit(X_train,y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"acc:\", acc)\n"
      ],
      "metadata": {
        "id": "92RFK25LQkEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad9e8e1-1ba5-483d-9df2-ca3417795d60"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.6343801235969226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Terzo modello: Logistic Regression\n",
        "Utilizziamo un modello di regressione logistica per il task di classificazione. La funzione utilizzata dal modello è la sigmoide:\n",
        "\\begin{equation}\n",
        "h_w(z) =Logistic(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\end{equation}\n",
        "\n",
        "#### **Perché utilizzare una Logistic Regression?**\n",
        "La Logistic Regression risolve i problemi che si riscontrano utilizzando una Linear Regression per fare classificazione: poiché la funzione di Threshold è linea netta, una regressione lineare divide troppo nettamente nelle due classi i nostri samples. Infatti, anche un sample molto vicino al *decision boundary* è indistinguibile in questo senso da sample che sono molto lontani dal bordo. Un modello di Logistic Regression invece, per come è costruita la funzione logistica, discrimina meglio tra punti che sono nettamente 0, nettamente 1, e casi limite sul bordo. "
      ],
      "metadata": {
        "id": "8csAEbprQIZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def logistic(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "x = np.linspace(-6, 6, 1000)\n",
        "y = logistic(x)\n",
        "plt.plot(x, y)\n",
        "plt.title('Funzione Logistica')\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('h')\n",
        "plt.ylim([0, 1])\n",
        "plt.yticks([0, 0.5, 1])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FFTwwbKBTgfC",
        "outputId": "3fa40423-0fac-4183-a5d6-28a9e8ae7e92"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Più precisamente, è possibile interpretare il numero tra 0 e 1 ottenuto come **probabilità** di appartenere alla classe 1."
      ],
      "metadata": {
        "id": "PFnCwTeZTfuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "class MyLogRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "    \n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)        \n",
        "    \n",
        "        for i in range(self.num_iterations):\n",
        "        \n",
        "            y_predicted = 1 / (1 + np.exp(-np.dot(X, self.weights))) \n",
        "            \n",
        "            grad = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "\n",
        "            self.weights -= self.learning_rate * grad\n",
        "            \n",
        "    def predict(self, X):\n",
        "\n",
        "        X = np.hstack([np.ones((X.shape[0], 1)), X]) \n",
        "\n",
        "        y_predicted = 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "\n",
        "        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "\n",
        "        return y_predicted_cls\n",
        "    \n",
        "    def sigmoid(self, x):\n",
        "        return \n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train]) \n",
        "\n",
        "reg = MyLogRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"acc:\", acc)\n"
      ],
      "metadata": {
        "id": "C6hx6ZmDQzad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a19e41-cd41-4966-cdc7-c633fc6c6721"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.6472442931012738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "nZUsVnIhQNML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1)  \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "reg = LogisticRegression()\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"acc:\", acc)"
      ],
      "metadata": {
        "id": "9Uaq8f0kQIvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c70bdde8-dc02-4f8b-e413-78240f49d76b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.6525413040736536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quarto Modello: K-nearest neighbors\n",
        "Partendo da un semplice modello di *lookup table* con entry $<sample, label>$ in cui, dato in input un sample x si restituisce, se esiste, la classe corrispondente, il KNN estende alla possibilità di restituire i **k sample che sono più vicini a x**. "
      ],
      "metadata": {
        "id": "AdvUW0cRWiYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificazione con KNN\n",
        "Per fare classificazione, prendiamo dai k vicini la label più ricorrente (majority voting). Utilizziamo un k dispari in modo che non possa esserci una situazione di pareggio."
      ],
      "metadata": {
        "id": "_frzcF2TXVW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "class MyKNNClassifier:\n",
        "  \n",
        "  def __init__(self, k):\n",
        "\n",
        "      if (k % 2 == 0):\n",
        "        print(\"k deve essere dispari!\")\n",
        "        exit()\n",
        "\n",
        "      self.lookupTable = []   # conterrà tutti i sample del training\n",
        "      self.k = k \n",
        "      self.distances = [] # conterrà le distanze tra i punti della lookup table\n",
        "      self.labels = None\n",
        "\n",
        "      \n",
        "  def fit(self, X, y):\n",
        "\n",
        "    # la lookup table deve contenere i sample del training: decido di usare direttamente X come lookup table\n",
        "    self.lookupTable = X\n",
        "    self.labels = y \n",
        "  \n",
        "  \n",
        "  def predict(self, samples):\n",
        "\n",
        "    result = []\n",
        "\n",
        "    # sulla riga i e la colonna j avremo la distanza euclidea tra il punto i di self.lookupTable\n",
        "    # e il punto j di samples\n",
        "    self.distances = cdist(samples, self.lookupTable, 'euclidean')\n",
        "\n",
        "    # ordiniamo in ordine crescente secondo la distanza\n",
        "    # argsort restituisce gli indici originali dell'array non ordinato, quindi\n",
        "    # tramite i primi k elementi otteniamo l'informazione su quali sono i corrispettivi \n",
        "    # indici per avere la lista ordinata\n",
        "\n",
        "    for e in range(len(samples)):\n",
        "      \n",
        "      k_indexes = (np.argsort(self.distances[e]))[:self.k]\n",
        "\n",
        "      # per ogni coppia (distanza, indice) nei primi k elementi di distances_ordinata prendiamo il secondo elemento (indice) e troviamo gli elementi\n",
        "      # della lookupTable che hanno quegli indici. Di questi elementi prendiamo solo la label\n",
        "      # (i valori del dizionario sono coppie (sample, label))\n",
        "      knn =  [self.labels.iloc[i] for i in k_indexes]\n",
        "\n",
        "\n",
        "      # vedo tra i k qual è la classe predominante e scelgo quella (major voting)\n",
        "      if (knn.count(0) > knn.count(1)):\n",
        "        result.append(0)\n",
        "      else:\n",
        "        result.append(1)\n",
        "\n",
        "      \n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "# normalizzazione\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "# k = 5 è solitamente un ottimo valore\n",
        "reg = MyKNNClassifier(k = 5)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "PX4tJFUMXZ5y"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Classificazione con KNN"
      ],
      "metadata": {
        "id": "FpNcQNp3I8jz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"acc:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74Z5h8N_I83I",
        "outputId": "613e68f8-6757-4980-94ce-d618f43cb498"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.6131920797074032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "xM2E9FKsKNPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "reg = KNeighborsClassifier(n_neighbors = 5)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"acc:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNXrcTzoKOyX",
        "outputId": "633a259a-05a1-41e8-9d4d-da44bbd2791e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 0.6131920797074032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressione con KNN\n",
        "Per fare regressione, prendiamo la mediana dei valori dei k vicini."
      ],
      "metadata": {
        "id": "tNHPSlxdXXBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class MyKNNRegressor:\n",
        "  \n",
        "  def __init__(self, k):\n",
        "\n",
        "      if (k % 2 == 0):\n",
        "        print(\"k deve essere dispari!\")\n",
        "        exit()\n",
        "\n",
        "      self.lookupTable = []\n",
        "      self.k = k \n",
        "      self.distances = []\n",
        "      self.labels = None\n",
        "\n",
        "      \n",
        "  def fit(self, X, y):\n",
        "    self.lookupTable = X\n",
        "    self.labels = y \n",
        "  \n",
        "  \n",
        "  def predict(self, samples):\n",
        "\n",
        "    result = []\n",
        "\n",
        "    # sulla riga i e la colonna j avremo la distanza euclidea tra il punto i di self.lookupTable\n",
        "    # e il punto j di samples\n",
        "    self.distances = cdist(samples, self.lookupTable, 'euclidean')\n",
        "\n",
        "\n",
        "    # ordiniamo in ordine crescente secondo la distanza\n",
        "    # argsort restituisce gli indici originali dell'array non ordinato, quindi\n",
        "    # tramite i primi k elementi otteniamo l'informazione su quali sono i corrispettivi \n",
        "    # indici per avere la lista ordinata\n",
        "\n",
        "    for e in range(len(samples)):\n",
        "      \n",
        "      k_indexes = (np.argsort(self.distances[e]))[:self.k]\n",
        "\n",
        "      # per ogni coppia (distanza, indice) nei primi k elementi di distances_ordinata prendiamo il secondo elemento (indice) e troviamo gli elementi\n",
        "      # della lookupTable che hanno quegli indici. Di questi elementi prendiamo solo la label\n",
        "      # (i valori del dizionario sono coppie (sample, label))\n",
        "      knn =  [self.labels.iloc[i] for i in k_indexes]\n",
        "\n",
        "\n",
        "      # calcolo la mediana tra i valori del knn \n",
        "      result.append(np.median(knn))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "# y non deve essere reso binario poiché si tratta di un task di regressione e non di classificazione \n",
        "y = df[\"shares\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "reg = MyKNNRegressor(k = 5)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "LHj9utSSXaOa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Regressione con KNN"
      ],
      "metadata": {
        "id": "PKowAscEJclU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GQA2nU2Jc4c",
        "outputId": "9e42df5c-4b78-4944-f361-b05bbdcf3a9f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 7978.197896209447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "Bc2DpZj6KPtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "reg = KNeighborsRegressor(n_neighbors = 5)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"rmse:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6pTUUWfKRd-",
        "outputId": "9554b036-dfca-4bf5-8e0a-d7dec29b86b6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 8860.240212263361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quinto Modello: Rete Neurale"
      ],
      "metadata": {
        "id": "vCG3K4K7LFhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regressione con Rete Neurale"
      ],
      "metadata": {
        "id": "I80Qa2ZiLLgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import math\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "\n",
        "alpha = 0.01\n",
        "epochs = 1000\n",
        "h = 116 \n",
        "n = X_train.shape[1]\n",
        "p = X_train.shape[0]\n",
        "n_output = 1\n",
        "\n",
        "W1 = np.random.rand(h, n)\n",
        "b1 = np.random.rand(1, h)\n",
        "W2 = np.random.rand(n_output, h)\n",
        "b2 = np.random.rand(1, n_output)\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "\n",
        "  # FEED FORWARD #\n",
        "\n",
        "  # per far tornare dimensionalmente i calcoli matriciali\n",
        "  b1_tiled = np.tile(b1, (p, 1)) # p x h\n",
        "  b2_tiled = np.tile(b2, (p, 1)) # p x n_output\n",
        "\n",
        "  A1 = sigmoid(np.dot(X_train, W1.T) + b1_tiled) # p x h\n",
        "\n",
        "  A2 = np.dot(A1, W2.T) + b2_tiled # p x n_outputs\n",
        "\n",
        "  y_pred = A2\n",
        "\n",
        "  # BACK PROPAGATION\n",
        "\n",
        "  sigmoid_derivative_A2 = sigmoid(A2) * (1 - sigmoid(A2)) \n",
        "\n",
        "  sigmoid_derivative_A1 = sigmoid(A1) * (1 - sigmoid(A1))\n",
        "\n",
        "\n",
        "  error = np.array(A2) - np.reshape(np.array(y_train), (X_train.shape[0], 1))\n",
        "\n",
        "\n",
        "  threshold = 1e-6 # th per il clipping\n",
        "\n",
        "\n",
        "  delta2 = np.clip(np.multiply(error, sigmoid_derivative_A2), -threshold, threshold) # p x n_output\n",
        "  delta1 = np.clip(np.multiply(np.dot(delta2, W2), sigmoid_derivative_A1), -threshold, threshold) # p x h\n",
        "  dW2 = np.dot(delta2.T, A1) # n_output x h\n",
        "  db2 = np.sum(delta2, axis=0, keepdims=True) # p x n_output\n",
        "  dW1 = np.dot(delta1.T, X_train) # h x n\n",
        "  db1 = np.sum(delta1, axis=0, keepdims=True) # p x h\n",
        "\n",
        "  # Aggiornamento dei pesi e dei bias\n",
        "  W2 = W2 - alpha * dW2\n",
        "  b2 = b2 - alpha * db2\n",
        "  W1 = W1 - alpha * dW1\n",
        "  b1 = b1 - alpha * db1\n",
        "\n",
        "  b1_tiled = np.tile(b1, (p, 1))\n",
        "  b2_tiled = np.tile(b2, (p, 1))\n",
        "\n",
        "  # Calcolo dell'errore di training\n",
        "  y_pred = np.dot(sigmoid(np.dot(X_train, W1.T) + b1_tiled), W2.T) + b2_tiled\n",
        "  error = math.sqrt(mean_squared_error(y_train, y_pred))\n",
        "\n",
        "\n",
        "  # Stampa dell'errore di training ogni 100 epoche\n",
        "  if i % 100 == 0:\n",
        "      print(\"Epoch:\", i, \"Error:\", error)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "b1_tiled = np.tile(b1, (X_test.shape[0], 1))\n",
        "b2_tiled = np.tile(b2, (X_test.shape[0], 1))\n",
        "\n",
        "A1_test = sigmoid(np.dot(X_test, W1.T) + b1_tiled)\n",
        "A2_test = np.dot(A1_test, W2.T) + b2_tiled\n",
        "\n",
        "y_pred_test = A2_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2i4Pyb6LNqJ",
        "outputId": "53e37251-0ed4-417d-87ad-fe79408e89a8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Error: 12854.348891877977\n",
            "Epoch: 100 Error: 12854.310481844104\n",
            "Epoch: 200 Error: 12854.272360931038\n",
            "Epoch: 300 Error: 12854.234523282928\n",
            "Epoch: 400 Error: 12854.196961825379\n",
            "Epoch: 500 Error: 12854.159670786692\n",
            "Epoch: 600 Error: 12854.122641375527\n",
            "Epoch: 700 Error: 12854.085862505486\n",
            "Epoch: 800 Error: 12854.049325858668\n",
            "Epoch: 900 Error: 12854.013028588246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Regressione con Rete Neurale"
      ],
      "metadata": {
        "id": "eO_zKzWOLbwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error = math.sqrt(mean_squared_error(y_test, y_pred_test))\n",
        "print(\"final error:\", error)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3GV0JtRLeXO",
        "outputId": "1b1b0529-265c-4517-f691-16f5c3ea5b64"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final error: 8448.850667699175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confronto con sklearn"
      ],
      "metadata": {
        "id": "A7Pypzm-Lg4V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "\n",
        "reg = MLPRegressor(hidden_layer_sizes = 60, activation = 'logistic', learning_rate_init = 0.1, random_state = 0, max_iter = 1000)\n",
        "reg = reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "\n",
        "print(math.sqrt(mean_squared_error(y_test, y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkJ86TDXLiGs",
        "outputId": "b5e8cd70-0d9d-4ae0-b14b-054e7b9d2593"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7647.452966632731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classificazione con Rete Neurale"
      ],
      "metadata": {
        "id": "w8k8BYYSLOHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "\n",
        "\n",
        "alpha = 0.1\n",
        "epochs = 1000\n",
        "h = 116 \n",
        "n = X_train.shape[1]\n",
        "p = X_train.shape[0]\n",
        "n_output = 2\n",
        "# n_output = 1 \n",
        "\n",
        "X_train = X_train.T\n",
        "X_test = X_test.T\n",
        "\n",
        "y_train = y_train.T\n",
        "y_test = y_test.T\n",
        "\n",
        "def init_params():\n",
        "    W1 = np.random.rand(h, n) - 0.5\n",
        "    b1 = np.random.rand(h, 1) - 0.5\n",
        "    W2 = np.random.rand(n_output, h) - 0.5\n",
        "    b2 = np.random.rand(n_output, 1) - 0.5\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / sum(np.exp(Z))\n",
        "    return A\n",
        "\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "   \n",
        "    # SHOULD I TILE B1???\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "\n",
        "    # SHOULD I CLIP?\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0\n",
        "\n",
        "def one_hot(Y):\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y\n",
        "\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ2 = A2 - one_hot_Y    \n",
        "    dW2 = (1 / p) * dZ2.dot(A1.T)\n",
        "    db2 = (1 / p) * np.sum(dZ2)\n",
        "\n",
        "  \n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = 1 / p * dZ1.dot(X.T)\n",
        "    db1 = 1 / p * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 = W1 - alpha * dW1\n",
        "    b1 = b1 - alpha * db1    \n",
        "    W2 = W2 - alpha * dW2  \n",
        "    b2 = b2 - alpha * db2    \n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 10 == 0:\n",
        "            print(\"Iteration: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "# def make_predictions(X, W1, b1, W2, b2):\n",
        "#     _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "#     predictions = get_predictions(A2)\n",
        "#     return predictions\n",
        "\n",
        "# def test_prediction(index, W1, b1, W2, b2):\n",
        "\n",
        "\n",
        "#     current_image = X_train[:, index, None]\n",
        "#     prediction = make_predictions(X_train[:, index, None], W1, b1, W2, b2)\n",
        "#     label = y_train[index]\n",
        "#     print(\"Prediction: \", prediction)\n",
        "#     print(\"Label: \", label)\n",
        "\n",
        "\n",
        "def test_prediction(X, W1, b1, W2, b2):\n",
        "\n",
        "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "\n",
        "    predictions = get_predictions(A2)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "W1, b1, W2, b2 = gradient_descent(X_train, y_train, alpha, epochs)\n",
        "\n",
        "y_previste = test_prediction(X_test, W1, b1, W2, b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "870196vILQ6x",
        "outputId": "d9175a56-9631-440e-9d20-ef40722e9fcd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration:  0\n",
            "[0 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.4913447895317673\n",
            "Iteration:  10\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5577171685322403\n",
            "Iteration:  20\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5693520416206842\n",
            "Iteration:  30\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5768563771086237\n",
            "Iteration:  40\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5842661201324294\n",
            "Iteration:  50\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5911713700141888\n",
            "Iteration:  60\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.5954280309002049\n",
            "Iteration:  70\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6001576541068895\n",
            "Iteration:  80\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6047296232066846\n",
            "Iteration:  90\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6071890272741605\n",
            "Iteration:  100\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6110988491250197\n",
            "Iteration:  110\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6141888696200536\n",
            "Iteration:  120\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6172788901150875\n",
            "Iteration:  130\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6197382941825634\n",
            "Iteration:  140\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.622607598927952\n",
            "Iteration:  150\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6249093488885386\n",
            "Iteration:  160\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6272110988491251\n",
            "Iteration:  170\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6297335645593568\n",
            "Iteration:  180\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6318461295916759\n",
            "Iteration:  190\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6332019549109255\n",
            "Iteration:  200\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6347154343370645\n",
            "Iteration:  210\n",
            "[1 0 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6359136055494246\n",
            "Iteration:  220\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.636985653476273\n",
            "Iteration:  230\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6377108623679647\n",
            "Iteration:  240\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6386567870093016\n",
            "Iteration:  250\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6395081191865049\n",
            "Iteration:  260\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6404855746492196\n",
            "Iteration:  270\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6411161910767775\n",
            "Iteration:  280\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6421882390036261\n",
            "Iteration:  290\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6430080403594514\n",
            "Iteration:  300\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6438278417152766\n",
            "Iteration:  310\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6441431499290556\n",
            "Iteration:  320\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6447737663566135\n",
            "Iteration:  330\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6453413211414157\n",
            "Iteration:  340\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6455935677124389\n",
            "Iteration:  350\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6460665300331073\n",
            "Iteration:  360\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6459719375689736\n",
            "Iteration:  370\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.646760208103421\n",
            "Iteration:  380\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6469493930316885\n",
            "Iteration:  390\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6471070471385779\n",
            "Iteration:  400\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6474223553523569\n",
            "Iteration:  410\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6477376635661359\n",
            "Iteration:  420\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.648021440958537\n",
            "Iteration:  430\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6482106258868043\n",
            "Iteration:  440\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6487781806716065\n",
            "Iteration:  450\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6488412423143622\n",
            "Iteration:  460\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6489988964212517\n",
            "Iteration:  470\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6490304272426297\n",
            "Iteration:  480\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6494087970991644\n",
            "Iteration:  490\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6494087970991644\n",
            "Iteration:  500\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6494718587419203\n",
            "Iteration:  510\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6493772662777866\n",
            "Iteration:  520\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6499763518839666\n",
            "Iteration:  530\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6503862525618792\n",
            "Iteration:  540\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6505123758473909\n",
            "Iteration:  550\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6506384991329024\n",
            "Iteration:  560\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6510483998108151\n",
            "Iteration:  570\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6512060539177046\n",
            "Iteration:  580\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6512060539177046\n",
            "Iteration:  590\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6513637080245941\n",
            "Iteration:  600\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6516159545956172\n",
            "Iteration:  610\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.651962793630774\n",
            "Iteration:  620\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6520258552735299\n",
            "Iteration:  630\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6519312628093962\n",
            "Iteration:  640\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6520573860949078\n",
            "Iteration:  650\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6524042251300647\n",
            "Iteration:  660\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6527195333438436\n",
            "Iteration:  670\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6527195333438436\n",
            "Iteration:  680\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6527825949865994\n",
            "Iteration:  690\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6526564717010878\n",
            "Iteration:  700\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6526880025224657\n",
            "Iteration:  710\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6527510641652216\n",
            "Iteration:  720\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6527510641652216\n",
            "Iteration:  730\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6528456566293552\n",
            "Iteration:  740\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6528141258079773\n",
            "Iteration:  750\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6526880025224657\n",
            "Iteration:  760\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6528456566293552\n",
            "Iteration:  770\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.65322402648589\n",
            "Iteration:  780\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6539177045562037\n",
            "Iteration:  790\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6539492353775815\n",
            "Iteration:  800\n",
            "[1 1 1 ... 0 1 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6524672867728204\n",
            "Iteration:  810\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6421251773608703\n",
            "Iteration:  820\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6389405644017027\n",
            "Iteration:  830\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6439539650007883\n",
            "Iteration:  840\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6475800094592464\n",
            "Iteration:  850\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6498817594198328\n",
            "Iteration:  860\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6514583004887278\n",
            "Iteration:  870\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6518051395238846\n",
            "Iteration:  880\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.652278101844553\n",
            "Iteration:  890\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6525618792369541\n",
            "Iteration:  900\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6528141258079773\n",
            "Iteration:  910\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530348415576226\n",
            "Iteration:  920\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530348415576226\n",
            "Iteration:  930\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530979032003784\n",
            "Iteration:  940\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530348415576226\n",
            "Iteration:  950\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6531294340217563\n",
            "Iteration:  960\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530348415576226\n",
            "Iteration:  970\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6530979032003784\n",
            "Iteration:  980\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6533816805927795\n",
            "Iteration:  990\n",
            "[1 1 1 ... 0 0 0] 21120    0\n",
            "10071    1\n",
            "23743    1\n",
            "30342    0\n",
            "23223    0\n",
            "        ..\n",
            "32399    0\n",
            "17048    1\n",
            "23924    0\n",
            "34086    0\n",
            "27439    0\n",
            "Name: shares, Length: 31715, dtype: int64\n",
            "0.6534447422355353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prestazioni Classificazione con Rete Neurale"
      ],
      "metadata": {
        "id": "r-2fPnm1Lrri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", get_accuracy(y_previste, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsnQeErzLsL7",
        "outputId": "2983247d-a581-40fb-da37-aabc12bc67db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 ... 1 0 1] 15945    0\n",
            "30835    0\n",
            "3840     1\n",
            "7414     1\n",
            "12457    0\n",
            "        ..\n",
            "2339     0\n",
            "12821    1\n",
            "28395    1\n",
            "12188    0\n",
            "2836     0\n",
            "Name: shares, Length: 7929, dtype: int64\n",
            "acc:  0.6498927985874637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confronto con sklearn"
      ],
      "metadata": {
        "id": "rBM0uNLZLoT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Versione di SKLearn\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"OnlineNewsPopularity/OnlineNewsPopularity.csv\")\n",
        "df = df.rename(columns=lambda x: x.strip())\n",
        "df = df.iloc[:, 2:]\n",
        "\n",
        "X = df.drop(\"shares\", axis=1) \n",
        "\n",
        "y = df[\"shares\"].apply(lambda x: 1 if x >= 1400 else 0)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test) \n",
        "\n",
        "\n",
        "reg = MLPClassifier(hidden_layer_sizes = 116, activation = 'logistic', learning_rate_init = 0.1, random_state = 0, max_iter = 1000)\n",
        "reg = reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZ5gytppLo8Y",
        "outputId": "ead21299-3941-4e8c-88b4-c9a8e181f816"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6365241518476479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br><br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "# APPUNTI DA SISTEMARE QUA SOTTO \n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ],
      "metadata": {
        "id": "RkOYFFWSMFOB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve3z7haozdzY"
      },
      "source": [
        "# Lezione del 10/03/2023\n",
        "\n",
        "\n",
        "Vediamo ora i modelli parametrici:\n",
        "\n",
        "vogliamo trovare h_hat(x) = y_hat che deve essere il più vicino possibile a h(x) = y. h(x) è l'ipotesi che abbiamo fatto, cioè abbiamo supposto che esista una funzione h a cui se dai i samples ti restiuisce la giusta label y\n",
        "\n",
        "(questo sia per modelli parametrici che non)\n",
        "\n",
        "I modelli parametrici assumono che h_hat dipende da una serie di parametri f(x) = h_hat(x; theta), con theta i parametri. Mettiamo \";\" tra x e theta perchè theta non è un parametro aggiuntivo di h ma è scelto una volta per tutte e utilizzato sempre (nello stesso modo in cui con i DT costruisco l'albero e poi uso l'albero con i dati su cui testo, mica lo ricambio). \n",
        "\n",
        "Esempio h_hat = 2x + 3 ha theta = (2, 3) \n",
        "\n",
        "\n",
        "INDUCTIVE BIAS: scegliendo h_hat introduciamo Inductive Bias (ma sta cosa la vedremo dopo)\n",
        "\n",
        "\n",
        "Ma come scegliamo theta? L'inferenza statistica se ne occupa. \n",
        "\n",
        "\n",
        "La funzione di Loss L: Y x R -> R^+ (una L in corsivo). Y nel caso di:\n",
        "\n",
        "- classificazione sarà un insieme di valori\n",
        "- regressione sarà un sottoinsieme di R\n",
        "\n",
        "Inoltre il secondo input è il risultato di h_hat e restituisce un valore maggiore > 0\n",
        "\n",
        "Un esempio di LOSS è il MSE:\n",
        "\n",
        "MSE = L(y, h_hat(x; theta)) = (y - h_hat(x;theta))^2\n",
        "\n",
        "Spesso viene calcolata sul dataset, quindi, se n è la dimensione del training, vale:\n",
        "\n",
        "L: Y^n x R^n -> R+^n \n",
        "\n",
        "Quindi la MSE, che deve restituire un valore solo, diventa:\n",
        "\n",
        "1/n sum((y - h_hat(x;theta))^2)\n",
        "\n",
        "\n",
        "Per cambiare la LOSS può variare solo theta, perché x sono i samples e y le label che sono già date. Quindi effettivamente LOSS è solo funzione di theta. L_hat(theta).\n",
        "\n",
        "Cerchiamo theta* t.c. minimizzo LOSS, quindi theta* = argmin_{theta}(L_hat(theta))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Dato un dataset D e un problema di classificazione, calcolo la probabilità che un etichetta sia uguale a y dato il sample x e i parametri theta (P{Y = y: X = x and theta}). Questa è la fase di test. Come scegliere theta?\n",
        "\n",
        "Nella fase di training vale:\n",
        "\n",
        "theta* = argmax_{theta}(P(D: theta))\n",
        "\n",
        "cioè devo trovare i parametri theta* che massimizzano la probabilità di osservare (o generare) il dataset D durante training. Se massimizzo questa probabilità massimizzo anche P{Y = y: X = x and theta} che riguarda il test, quindi la nostra probabilità di azzeccare dati che non fanno parte del training D. Perciò se miglioro il training miglioro le predizioni\n",
        "\n",
        "Questo metodo di trovare i parametri theta* si chiama max likelihood estimation (stimatore di massima verosimiglianza)\n",
        "\n",
        "\n",
        "Riassumendo: dobbiamo minimizzare la loss (visione di ottimizzazione) oppure massimizzare la likelihood (visione probabilistica). Vedremo che queste due cose sono la stessa cosa vista da due punti di vista diversi.\n",
        "\n",
        "Empirical Risk Minimization è il principio che sta alla base di quello che abbiamo appena detto: per trovare l'ipotesi che generalizza nella maggior parte dei casi non posso fare altro che minimizzare il rischio empirico, quello empiricamente valutato su un dataset di training. Il mio dataset è quindi rappresentativo del task che voglio apprendere (non è banale ottenere un insieme di training che sia rappresentativo). In statistica selezionare il campione (dataset) è però molto più importante, in ML abbiamo così tanti dati che li usiamo tutti e quindi shalla.\n",
        "\n",
        "\n",
        "assumiamo che il campione sia sempre rappresentativo, cioè gli esempi futuri (quelli su cui testo) saranno sempre gli esempi passati (quelli su cui traino) e quindi dati di train e dati di test seguono la stessa distribuzione.\n",
        "\n",
        "Assunzioni quindi nel ML:\n",
        "\n",
        "(1) IDENTICALLY DISTRIBUTED: il dataset contiene sample che provengono tutti dalla stessa distribuzione di probabilità. Non posso aver un sample campionato da una distribuzione che favorisce i cani ai gatti e un altro in cui i cani sono più frequenti dei gatti. Quindi ogni probabilità è identica. Quindi non posso avere due sottodistribuzioni in un campione rappresentativo (non li vediamo ma nei mixture model questa cosa di più distribuzioni si può fare)\n",
        "\n",
        "(2) INDIPENDENZA: samples indipendenti. Conoscere un sample non porta a conoscere meglio gli altri sample del dataset. Esempio: sondaggio politico fuori da scuola, intervisto madre e figlio, è chiaro che non siano indipendenti perché stanno nella stessa casa e c'è un influenza, ma noi non la consideriamo.\n",
        "\n",
        "Quindi, se un processo che genera il dataset è IID (segue (1) e (2)), posso scrivere P{D:theta} = produttoria(P{x_i, y_i: theta}). Che è semplificato.\n",
        "\n",
        "D'ora in avanti assumeremo sempre IID\n",
        "\n",
        "\n",
        "scomponi D in D_train e D_test\n",
        "\n",
        "partiziona D_train in k parti (buoni valori per k sono 5 e 10)\n",
        "\n",
        "for combinazione in combinazioni(iperparametri)\n",
        "\n",
        "  for i in range(k) \n",
        "    \n",
        "    D_validation = D_train[i]\n",
        "\n",
        "    traino sulle (k-1) parti non validation\n",
        "\n",
        "    calcolo accuracy su D_validation \n",
        "\n",
        "  accuracy = accuracy media delle k iteraz\n",
        "\n",
        "  if (accuracy > current_accuracy)\n",
        "    la nuova combinazione di iperparametri è migliore rispetto alla corrente\n",
        "\n",
        "\n",
        "Abbiamo trovato la migliore combinazione di parametri, adesso testiamo il modello con questi parametri su D_test e vediamo l'accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# LEAVE ONE OUT CROSS VALIDATION:\n",
        "\n",
        "se k fold e k = numero elementi levo un elemento ad ogni iterazione e lo uso per fare validation\n",
        "\n",
        "\n",
        "\n",
        "# Model Selection: selezionare parametri e iperparametri. Noi l'abbiamo fatto con K-Fold cross validation\n",
        "\n",
        "Nell'ottimizzazione abbiamo una funzione e conosciamo dominio della funzione e vincoli, troviamo il punto che minimizza/massimizza la funzione.\n",
        "\n",
        "In ML oltre all'ottimizzazione abbiamo tutta la parte prima: seleziona modello, seleziona iperparametri, seleziona come dividere tra train, test e validazione.\n",
        "\n",
        "# SUL LIBRO CI SONO GLI ALGORITMI SCRITTI BENE PER TUTTA STA ROBA. Sul libro c'è ERROR-RATE, che è la LOSS ma potrebbe anche essere l'accuracy o altro.\n",
        "\n",
        "La funzione di Error rate sul libro:\n",
        "\n",
        "guarda a sinistra il grafico relativo ai DT. Sappiamo che all'aumentare dei nodi (e quindi dell'altezza), il modello fa sempre meglio sui dati di training (fino ad arrivare all'overfitting), questo significa che l'errore sul training set diminuisce (curva verde). Il validation set invece (curva rossa), all'inizio diminuisce, ma ad un certo punto, quando il modello overfitta, l'errore sul validation set aumenta. Tipicamente quello che si fa (non lo faremo nell'homework) è trovare qual è il valore dopo cui il validation set aumenta, quindi fermarsi a quel valore là, e usare quello.\n",
        "\n",
        "A destra c'è invece il grafico su reti neurali. Anche qui a una certa il validation set error reinizia ad aumentare, però poi ridiminuisce di nuovo all'aumentare dei neuroni (thousand of parameters). La domanda dei ricercatori è: come mai le reti neurali generalizzano così bene nonostante l'alto numero di parametri?\n",
        "\n",
        "# Complessità di un Modello (es. numero di parametri, nodi in un DT...)\n",
        "\n",
        "Dobbiamo tenerla a un livello t.c. il rischio di overfitting sia basso.  \n",
        "\n",
        "In generale dobbiamo avere validation set error e traning set error abbastanza vicini, altrimenti o c'è overfitting o c'è un bug.\n",
        "\n",
        "# Torniamo alla funzione di LOSS:\n",
        "\n",
        "L(spam, nospam): modello per vedere se mail è spam o no. Vediamo l'etichetta prodotta dal modello e da un umano. Se sono uguali è 0, se sono diverse è 1.\n",
        "\n",
        "Ci sono vari esempi di Loss Function, esempio:\n",
        "\n",
        "- Absolute-Value Loss: misura la distanza in v.a. tra i due valori\n",
        "\n",
        "- Squared-Error Loss che è MSE ma senza la media.\n",
        "\n",
        "- 0/1 Loss: è quella che si fa per task di classificazione. 0 se sono uguali 1 se sono diverse (quella che abbiamo fatto sopra).\n",
        "\n",
        "La loss indica quanto sono felice con il risultato del mio modello.\n",
        "\n",
        "\n",
        "GENERALIZATION LOSS: valore atteso del Loss sul dataset D al variare del dataset D. \n",
        " \n",
        "E[LOSS su D] al variare di D è la generalization loss.\n",
        "\n",
        "ma non posso calcolare la G.L. perché non so D, non so generare il dataset. Allora si prendono un po' di dati di D e si calcola la media campionaria delle LOSS su D.  \n",
        "Questa media campionaria è uno stimatore buono del valore atteso che rappresenta la G.L.\n",
        "\n",
        "1/n sum(L) al variare di D\n",
        "\n",
        "Nel nostro caso la Loss empirica corrisponde al caso in cui usiamo un solo sample D, quindi si riduce a L, oppure potremmo usare k fold cross validation e quindi usare k sample.\n",
        "\n",
        "In generale G.L. e E.L. saranno diversi per 4 motivi: non realizzabilità, varianza, rumore, complessità computazionale di esplorare tutti i possibili valori nello spazio delle ipotesi H.\n",
        "\n",
        "Realizzabilità: quando un problema di ML è realizzabile? Realizzabile se nello spazio delle ipotesi H ce ne è una, h, che genera i possibili dataset D.\n",
        "\n",
        "Varianza: se prendo un dataset D_1 e cerco l'ipotesi migliore h_1, poi prendo un dataset D_2 e cerco l'ipotesi migliore h_2, la varianza è la differenza tra h_1 e h_2. Se c'è alta varianza non ho sicurezza che il modello vada bene per dei dati che non ho ancora visto. Se invece la varianza è bassa il modello va bene.\n",
        "\n",
        "Noise: le etichette y è possibile che siano generate con un po' di rumore. y = f(x) + eps\n",
        "\n",
        "f(x) = natura, cioè la funzione che genera i dati\n",
        "epsilon = rumore\n",
        "\n",
        "Il rumore, essendo caos, non si può predirre. Ma posso porre delle ipotesi sul rumore: es. rumore gaussiano con media 0 e varianza sigma^2\n",
        "\n",
        "Complessità computazionale: se lo spazio H è molto grosso cercare la h migliore potrebbe non essere facile.\n",
        "\n",
        "# Penalizing Complex Hypothesis (Regolarizzazione)\n",
        "\n",
        "Prendo un modello che può essere complesso a piacere. Impongo una penalizzazione sulla complessità: la LOSS oltre a dirti quanto hai sbagliato, contiene un elemento (lambda complexity) che penalizza la complessità del modello. In questo quando minimizzo la Loss minimizzo automaticamente anche la complessità del modello. \n",
        "\n",
        "Due funzioni di regolarizzazione L1 e L2 sono in realtà degli iperparametri (DA CONTROLLARE, NON SONO SICURO)\n",
        "\n",
        "\n",
        "# Metriche per valutare la qualità dei modelli addestrati. Ci sono metriche per la classificazione e per la regressione:\n",
        "\n",
        "Negli homework non dobbiamo usarle tutte, vuole solo accuracy per classificazione e RMSE per regressione.\n",
        "\n",
        "Un dataset è sbilanciato se ci sono molti più samples di una classe che di un altra, esempio 99% dei samples è di classe y = 1 e 1% y = 0. \n",
        "Accuracy non è una buona metrica se il dataset è sbilanciato perché, riprendendo il suddetto esempio: nel caso del 99% y = 1, un modello che dice sempre e comunque y = 1 allora quel modello ha il 99% di accuracy ma è ovviamente falsocco.\n",
        "\n",
        "\n",
        "\n",
        "Matrice di confusione: metodo per investigare la natura degli errori. \n",
        "\n",
        "\n",
        "Precision: rapporto tra numero di volte che predico correttamente positivo e numero di volte in cui ho detto positivo\n",
        "\n",
        "Recall: rapporto tra numero di volte che predico correttamente positivo e positivi totali (TP + FN). Quindi è un errore sui positivi.\n",
        "\n",
        "F1 Score è media armonica di Precision e Recall\n",
        "\n",
        "\n",
        "\n",
        "ROC curve: se ho y = P{x, y} quindi una probabilità compresa tra 0 e 1, ma ho un modello binario che accetta solo classe = 0 oppure 1, come faccio a rendere binario y?\n",
        "\n",
        "Posso dire y > treshold -> y = 0 altrimenti 1\n",
        "\n",
        "La ROC curve mostra i valori di FPR e TPR al variare della treshold\n",
        "\n",
        "\n",
        "\n",
        "Precision Recall Curve: il grafico mostra come varia la recall al variare della precision. Prendo la curva che è alta sia per Precision che per Recall. Perchè voglio sia essere preciso (Precision) e anche trovarne tanti (Recall).\n",
        "\n",
        "Es: test per il covid. Precision dice quante volte la persona ha il covid sapendo che il test è venuto positivo. Recall dice quante volte uno con il covid risulta positivo. Voglio entrambi alti.\n",
        "\n",
        "\n",
        "MAE: media delle differenze.\n",
        "\n",
        "mean_absolute_error(actual, predicted) \n",
        "actual è l'array con la ground truth e predicted quelli ritornati dal modello.\n",
        "\n",
        "\n",
        "MSE: lo conosciamo\n",
        "\n",
        "RMSE: la radice quadrata della MSE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hGywa0ePz6E"
      },
      "source": [
        "METODI DI DISCRETIZZAZIONE\n",
        "\n",
        "PROVIAMONE DIVERSI, POSSIAMO PROVARE A NON DISCRETIZZARE (DOPPIO CICLO FOR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANhvTYwbN3J8"
      },
      "source": [
        "# Lezione 13/03/2023\n",
        "La regressione lineare h(x) = w_1x + w_0\n",
        "Il dato x ha una sola dimensione quindi ha 1 sola feature.\n",
        "\n",
        "Cerchiamo la retta che passa il più vicino possibile a tutti i dati. \n",
        "\n",
        "h(x) Prodotto scalare tra vettore di pesi e vettori di feature\n",
        "\n",
        "aggiungiamo una feature fittizia uguale a 1 che rappresenta il valore da moltiplicare per il bias w_0\n",
        "\n",
        "Usiamo la funzione di Loss MSE. Questa penalizza le rette che passano lontane dai punti con dipendenza quadratica dalla distanza. Il minimo della loss esiste sempre ed è unico. Una funzione convessa infatti (come quella di Loss applicata al modello) ammette sempre minimo ed è unico. Per calcolare il minimo prendiamo le derivate parziali rispetto ad ogni parametro, cioè calcoliamo il gradiente. Troviamo poi il vettore per cui il gradiente si annulla.\n",
        "\n",
        "\n",
        "\n",
        "Cosa succede quando la funzione di Loss non è però convessa? Usiamo l'algoritmo di discesa del gradiente: il gradiente è un vettore che punta sempre nella direzione di massima crescita. Se io vado nella direzione opposta descresco il più possibile. L'algoritmo sta sul libro:\n",
        "\n",
        "w = un punto a caso nello spazio dei parametri\n",
        "while gradiente non arriva a convergenza:\n",
        "  per ogni parametro w_i in w:\n",
        "    aggiorno ogni parametro sottraendo dal valore corrente un pochino del gradiente nella direzione relativa a quel parametro:\n",
        "    da w_i sottraggo alpha volte la derivata parziale della loss rispetto a w_i\n",
        "\n",
        "Alpha è il learning rate: immaginiamo che la funzione sia una valle, se salto troppo vado dall'altra parte della valle, ma io voglio trovare il punto minimo! Quindi con alpha faccio piccoli passi. Noi alpha lo consideriamo fisso.\n",
        "\n",
        "\n",
        "Come si fa ad applicare l'algoritmo nel caso in cui la regressione lineare sia univariata? Cioè abbiamo solo due parametri w_0 e w_1. w_0 = w_0 + alpha(...) questo è come vieen updatato il bias\n",
        "\n",
        "mentre w_1 = w_1 + alpha(...)*x\n",
        "\n",
        "\n",
        "\n",
        "Aggiornare i pesi fa ruotare le rette fino a che non si trova la figura arancione più marcata che è il valore dell'ottimo (guarda figura).\n",
        "\n",
        "\n",
        "La loss per ora l'abbiamo calcolata su un solo punto x, bisognerebbe calcolarla su ogni x nel dataset (classico algoritmo del gradient descent) oppure si fa batch gradient descent: abbiamo 1000 punti prendiamo 10 alla volta e per ogni gruppo di 10 calcolo la loss, aggiorno i parametri, prendo il secondo gruppo di 10 applico di nuovo e così via. Aggiorno 100 volte il mio gradiente.\n",
        "\n",
        "Convergenza: i valori dei pesi non cambiano più o cambiano talmente poco che trascuriamo. Poiché non è sicuro che dopo i primi 1000 siamo a convergenza: si riparte con il primo gruppo di 10 e via ancora. Ogni passata sui 1000 dati è un epoca. \n",
        "\n",
        "Un caso particolare di batch è lo stochastic gradient descent: \n",
        "\n",
        "CONTROLLA SU LIBRO: sembrerebbe che il libro dica che il batch viene fatto su tutti mentre lo stochastic è quello che lui ha definito come batch\n",
        "\n",
        "\n",
        "\n",
        "Se la funzione di Loss è convessa non ci sono problemi a trovare il minimo: pensa di essere una pallina che scende per una valle, una volta che ti fermi naturalmente sei nel punto di minimo. \n",
        "\n",
        "Ma se L non è convessa (es. ha diversi su e giù, \"diverse valli\") allora la pallina si ferma in un punto che è sicuramente minimo locale ma non è detto che sia minimo globale, che è quello che cerchiamo.\n",
        "\n",
        "Non si sa ancora la dimostrazione matematica: ma si sa che il minimo locale che si trova non è tanto diverso dal minimo globale, quindi ci va bene.\n",
        "\n",
        "\n",
        "#Soluzione Analitica:\n",
        "\n",
        "Sia X matrice dei dati con d colonne e n righe: abbiamo d feature e n sample.\n",
        "\n",
        "Sia y vettore degli output per il traning, con n righe e 1 collonna\n",
        "\n",
        "Sia w vettore dei pesi, con ultima componente w_0 che è il bias\n",
        "\n",
        "Allora y^ è il vettore degli output predetti: y^ = Xw \n",
        "\n",
        "Quindi la Loss L(w) = (||y^ - y||)^2 diventa \n",
        "L(w) = (||Xw^ - y||)^2\n",
        "\n",
        "\n",
        "Il gradiente vale: (Prendiamo per buono senza calcolarcelo dice il prof., non serve saperla)\n",
        "\n",
        "grad(L(w)) = 2X^T(Xw-y) \n",
        "\n",
        "Lo settiamo a zero trovando l'espressione di w* (guarda slide)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y25gTY4XdDJU"
      },
      "source": [
        "\n",
        "# Regolarizzazione per Regressione Lineare:\n",
        "\n",
        "Lambda è un iperparametro: lo decidiamo noi. Più è grande lambda meno conta l'errore che viene compiuto sulla classificazione (cioè trascurabile emploss). \n",
        "Per impostare il valore di lambda si usa il validation dataset (come abbiamo fatto con l'altezza dell'albero).\n",
        "\n",
        "Complexity in questo caso può essere questa semplice funzione: sum(|w_i|^q)\n",
        "\n",
        "Chiamata Loss Q o Norma Q (non ho capito quale delle due o se sono sinonimi)\n",
        "\n",
        "Le versioni più famose hanno Q = 1 (regolarizzazione L1) e Q = 2 (regolarizzazione L2)\n",
        "\n",
        "I seguenti effetti valgono per tutti i modelli, non solo per linear regression:\n",
        "\n",
        "Regolarizzazione L1: l'effetto è di azzerare alcuni dei parametri (è come se selezionassi solo i parametri maggiormente importanti per decidere l'output e gli altri li mettessi a 0). Qui l'obiettivo è di trovare quali sono i parametri più importanti\n",
        "\n",
        "Regolarizzazione L2: l'effetto è di rendere piccoli i parametri, cioè normalizzarli, disponendoli su un cerchio di raggio 1. Qui l'obiettivo è di non avere parametri molto più grandi di altri.\n",
        "\n",
        "- Applicando L2 a regressione lineare è Ridge Regression: L = MSE + lambda*L_2. Con ridge regression posso ancora scrivere w* in forma chiusa (sul libro/slide, p è il numero di feature, io l'ho chiamato d)\n",
        "\n",
        "\n",
        "- Applicando L1 a regressione lineare è Lasso Regression: L = MSE + lambda*L_2 \n",
        "Anche qui esiste forma chiusa per w* ma non è elegante e non la mostriamo. è come se io applicassi un operatore di selezione delle features. Tramite Lasso Regression trovo quali sono le feature non importanti (hanno peso = 0)\n",
        "\n",
        "\n",
        "- Applicando sia L1 che L2 a regressione lineare è Elastic Net, usiamo lambda 1 e lambda 2 rispettivamente. Possiamo anche usare un singolo iperparametro alpha.\n",
        "\n",
        "\n",
        "\n",
        "Secondo passo homework: regressione lineare non considerando più shares < 1400 ma considerando il vero valore degli shares. Vuole usare regressione lineare senza regolarizzazione e poi con tutti e tre i tipi\n",
        "\n",
        "\n",
        "\n",
        "# COSE DA VEDERE: ESCLUSIONE DEGLI OUTLIER\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvO2FjNTAXDM"
      },
      "source": [
        "[[[]\n",
        "# La classificazione può essere fatta anche con il modello di regressione lineare\n",
        "[ex[xx\n",
        "Ogni punto dello spazio ha oltre una posizione anche un colore (un etichetta). Quindi non vogliamo più la retta che si avvicina di più ai punti ma che meglio divide i punti con un etichetta dall'altra.\n",
        "\n",
        "Terzo passo homework: prendere la regressione lineare del secondo punto e usarla per classificazione. Qual è il valore dei parametri che fa passare la retta esattamente tra i punti positivi e negativi (labels).\n",
        "\n",
        "Vuole semplicemente che ci proviamo non è facilissimo trovare sta retta.\n",
        "\n",
        "Vuole che controlliamo se tutti i punti con etichetta 1 abbiamo valore delle feature che una volta sostituiti nel modello risultino in un valore del modello > 0 e viceversa con etichetta 0 (< 0). Scritta vettorialmente: vogliamo trovare w t.c. la mia ipotesi h valga h_w(x) = 1 if w*x >= and 0 otherwise\n",
        "\n",
        "L'ipotesi h è treshold: h(w,x) = 1 se wk>= 0 and 0 otherwise\n",
        "\n",
        "Come faccio a ottimizzare? Non posso applicare la regressione lineare non avendo un valore ottimo. Posso usare gradient descent. Aggiusto il valore dei pesi w finché la Loss non è minimizzata. Come Loss usiamo la treshold. Dobbiamo calcolare la derivata della treshold (w_i = w_i + alpha(y-h)x_i). In questo caso y o è 0 o è 1. Se predico un valore > 0 ma y = 0 (cioè avrei dovuto predire 0) la differenza è molto alta, se predico < 0 ma y = 1 ancora differenza molto alta. Quindi Loss è molto alta se predico male.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCTl1pLyzqeL"
      },
      "source": [
        "# Terzo Modello LOGISTIC REGRESSION\n",
        "\n",
        "Quarto passo homework: implementare regressione logistica.\n",
        "\n",
        "La curva logistica a forma di S può essere usata al posto della regressione lineare per fare classificazione.\n",
        "\n",
        "L'ipotesi con linear r. era Threshold(w*x)\n",
        "Con logistic r. l'ipotesi è Logistic(w*x) = 1 / (1 + e^wx)\n",
        "\n",
        "\n",
        "quindi c'è sempre il prodotto scalare tra w e x ma tutto intorno c'è la funzione logistica.\n",
        "\n",
        "La funzione varia tra 0 e 1, quasi sempre molto vicino a 0 e poi tende a 1 molto velocemente.\n",
        "\n",
        "Come usare questa funzione per decidere se un punto ha y = 0 o y = 1. Se il valore ritornato è superiore alla threshold value (tipicamente 1/2).\n",
        "\n",
        "Nel caso di classificazione logistic regression è quasi sempre il migliore. \n",
        "\n",
        "Quando abbiamo un problema che affrontiamo per la prima volta e vogliamo avere idea delle prestazioni del classificatore usare la regressione logistica.\n",
        "\n",
        "Problema: la funzione Logistic è convessa. Prendiamo sempre Loss con MSE. Non esiste forma chiusa per quando il gradiente è = 0. Si deve usare per forza gradient descent per fare aggiornamento pesi (vanilla su tutti i dati, batch oppure stochastic). La regola per l'update sta sulle slide\n",
        "\n",
        "La MSE non è la Loss migliore in questo caso, quella che viene usata è la Binary Cross Entropy. Se y = 1 rimane solo il primo addendo, per y = 0 solo il secondo. In questo caso l'update rule è molto più semplice.\n",
        "\n",
        "\n",
        "Perché Regressione Logistica è lineare? Sfruttiamo il concetto di odd ratio P(X) / (1 - P(x))\n",
        "\n",
        "Cioè la probabilità che qualcosa sia 1 / prob che sia 0\n",
        "\n",
        "log-odds = logaritmo(odd ratio)\n",
        "\n",
        "se modello log-odds come funzione lineare e tiro fuori p(x) trovo che è la logistic function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3qruqfav0nu"
      },
      "source": [
        "# KNN ed i modelli non parametrici\n",
        "Non Parametric Model: \n",
        "\n",
        "esempio Piecewise che è un sistema di apprendimento basato sulle istanze, nella maggior parte dei casi si chiamano modelli basati su memoria perchè hanno bisogno di tenere in memoria i samples che hanno visto durante il training set. \n",
        "\n",
        "\n",
        "Modello più semplice per questo apprendimento table lookup:\n",
        "\n",
        "x   |  y\n",
        "x_0    y_0\n",
        "...    ...\n",
        "\n",
        "\n",
        "Se trova x_0 allora restuisce y_0\n",
        "Questo modello overfitta ovviamente e non sa cosa fare per oggetti che non ha nella tabella.\n",
        "Non si tratta di un buon algoritmo di Learning.\n",
        "\n",
        "\n",
        "Come posso modificarlO:\n",
        "\n",
        "mi arriva X che non ho mai visto prima, allora cerco la più vicina nella colonna X. Per regressione invece prendo i k più vicini e ad esempio faccio la media.\n",
        "\n",
        "Questo è l'algoritmo di machine learning più semplice che esista. Non c'è da fare nessun apprendimento in realtà.\n",
        "\n",
        "Homework: prossimo passo è usare k-nearest neighbours (i k più vicini) per fare classificazione e regressione. \n",
        "\n",
        "NOTA: k dispari (sia per classificazione sia per regressione, in realtà è importante solo per la classificazione per non avere situazioni di pareggio).\n",
        "\n",
        "Per classifciazione majority voting\n",
        "\n",
        "Per regressione fare la media\n",
        "\n",
        "\n",
        "Come trovare i k più vicini usare la distanza euclidea. Variare la distanza ha impatto nelle prestazioni degli algoritmi. La più generica è la Minkoski.\n",
        "C'è anche la distanza del Coseno.\n",
        "\n",
        "\n",
        "Per la classifi cazione non è per forza necessario avere una line retta, poteri avere anche un polinomio che mi divide il dataset > 0 dal dataset < 0. Nel caso dei k vicini infatti per classificazione non si usa una linea retta.\n",
        "\n",
        "Infatti dalla figura 19.10 del libro vedi che la nuvoletta in grigio è quello considerato positivo.\n",
        "\n",
        "Per k = 5 la nuvoletta è liscia e non frastagliata/fratturata perché è migliore.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Data Normalization: nel momento in cui le feature hanno valori ben compresi in un certo range gli algoritmi funzionano meglio. Quando le feature sono normalizzate allora l'algoritmo apprende prima e meglio. Ci sono modelli per cui questa cosa è indifferente: es. alberi. Regressione lineare e logistica invece avere feature normalizzate aiuta\n",
        "\n",
        "Normalizzare significa dividere per la norma. Questa è la geometrica\n",
        "La normalizzazione statistica consiste nel rendere l'elemento una variabile casuale di media mu e varianza sigma^2. Prendo tutti i valori di una feature calcolo media e varianza, prendo per ogni  valore della feature sottraggo per la media e divido per la varianza.\n",
        "E ottengo i valori normalizzati.\n",
        "\n",
        "Faccio questo per ogni feature.\n",
        "\n",
        "\n",
        "\n",
        "KNN funziona quando abbiamo tanti dati e poche feature.\n",
        "Problema: se ho tante feature le distanze diventano tutte molto grandi e simili quindi indistinguibili. \n",
        "\n",
        "La maledizione dell'alta dimensionalità: supponiamo di avere un quadrato di lato 1\n",
        "Il più piccolo quadrato che contiene tutti i suoi k più vicini è il kn group (?)\n",
        "\n",
        "Ho N oggetti\n",
        "\n",
        "Dato un oggetto cerco il più piccolo quadrato che contiene k vicini. \n",
        "\n",
        "L'area di questo quadrato è L^d = k / N quindi formula inversa con radice d-esima\n",
        "\n",
        "L è il lato del quadrato minimo\n",
        "\n",
        "(c'è un errore sulle slide mi sa)\n",
        "\n",
        "In altre parole se tiro a caso qual è la probabilità di finire nel quadrato di lato L?\n",
        "\n",
        "(GUARDA CODICE MARKDOWN CHE SI VEDE BENE IL QUADRATO)\n",
        "\n",
        "--------------------------------\n",
        "|\n",
        "|\n",
        "!\n",
        "!\n",
        "!  ---- \n",
        "!  |  |\n",
        "!  ----\n",
        "!\n",
        "!\n",
        "!\n",
        "!\n",
        "!-------------------------------\n",
        "\n",
        "\n",
        "La curva (sulle slide) mostra la grandezza del quadrato all'aumentare delle dimensioni\n",
        "\n",
        "Con 200 dimensioni (feature) il quadrato è il 94% del quadrato quindi riempie tutto lo spazio e quindi tutto sta vicino\n",
        "\n",
        "\n",
        "Questo era un intuizione, poi dopo c'è la funzione vera e propria che rappresenta le performance del modello al crescere delle features. Molto simile alla funzione che c'è dopo che misura il volume di una ipersfera al crescere delle dimensioni.\n",
        "\n",
        "\n",
        "KNN del nostro homework si aspetta che non funzioni benissimo (ma non è sicuro) \n",
        "\n",
        "\n",
        "\n",
        "Altro problema del KNN:\n",
        "\n",
        "Se devo confrontare un punto e ho 100 samples nella tabella di lookup devo calcolare le distanze con tutti i 100 quindi il tempo di inferenza (esecuzione) è O(n) dove n è il numero di sample.\n",
        "\n",
        "In realtà il vero costo se ho vettori di dimensione d è O(n*d) perché una x_i ha d componenti.\n",
        "\n",
        "\n",
        "k-d-tree prendo un quadrato ogni quadrante lo divido in 2 poi ogni mezzo quadrante lo divido in 22 e così via.\n",
        "\n",
        "Il costo  O(log(n)).\n",
        "\n",
        "\n",
        "\n",
        "Calcolare la distanza tra due punti: voglio progettare una funzione di hashing \n",
        "\n",
        "\n",
        "Per approssimare questa distanza vado a costruire una funzione di has t.c. la probabilità che i due punti siano vicini (cioè che la distanza sia piccola) è uguale (FORSE E' DIVERSO?????) alla probabilità di collidere nello spazio di hashing. Cioè voglio trovare la funzione h di hashing t.c. la probabilità di collidere dei due punti x e y sia uguale alla distanza tra i due punti d(x,y) \n",
        "\n",
        "Quindi prendo x e y e attraverso h li vado a mappare in due punti. Voglio che la probabilità che i due punti finiscano nello stesso bucket sia uguale alla similarità dei due punti. Quindi due punti molto vicini finiscono nello stesso bucket.\n",
        "\n",
        "Calcolare h è costante (hashing)\n",
        "\n",
        "Per capire il più vicino mi costruisco una struttura dati che è divisa in bucket (una lista di dimensione diciamo p)\n",
        "\n",
        "una funzione hash mappa nei bucket i punti\n",
        "\n",
        "Con N oggetti\n",
        "\n",
        "Ci saranno delle collisioni ma se sono bravo posso mettere in ogni bucket N/p punti\n",
        "\n",
        "\n",
        "Quando mi arriva un nuovo oggetto faccio lo hashing e le distanze le calcolo solo all'interno del bucket in cui mi trovo. Così devo fare solo N/p confronti.\n",
        "\n",
        "Quindi il csoto è  O(N/p*d)\n",
        "\n",
        "Questo serve per valutare i k più vicini. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Prossimo argomento: regressione non parametrica\n",
        "\n",
        "4 esempi di regressione:\n",
        "\n",
        "1. Connect the Dots: passa per ogni punto con un segmento. Questa soluzione non funziona quando i punti sono messi proprio a caso (c'è molto rumore). Perché così zigzago incontrollatamente.\n",
        "\n",
        "2. KNN Regression: non ti accontenti di costruire i segmenti ma passo tra i più vicini, cioè per ogni punto guardo i vicini ci faccio passare una retta con il coefficente medio dei miei vicini (controlla). Essenzialmente per ogni punto quel punto è la media dei vicini\n",
        "\n",
        "3. KNN linear regression: per ogni punto approssimo il valore di quel punto attraverso la linea che meglio passa per i suoi vicini (ricontrolla).\n",
        "\n",
        "4. Locally W. Regression: dato un nuovo punto x andiamo a interpolare con un peso , quale sarebbe la retta migliore per quel punto che minimizza MSE pesato. Pesato in che senso? Peso maggiormente l'errore fatto rispetto ai punti vicini.\n",
        "\n",
        "Uso un MSE particolare: sum(k(x x_i)(...) GUARDA SUL LIBRO\n",
        "\n",
        "con x_i punto corrente del dataset\n",
        "\n",
        "con k similarità tra x e e x_i (controlla)\n",
        "\n",
        "\n",
        "k(x, xi) = e^... è una possibilità di similarità\n",
        "\n",
        "oppure ce ne è un'altra con un max\n",
        "\n",
        "\n",
        "\n",
        "NOTA: sul libro usa SE e non MSE perchè tanto poi devi fare la derivata e minimizzare quindi non conta nulla essendo / n costante (non c'entrava nulla adesso però gliel'ho chiesto)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "y25gTY4XdDJU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}